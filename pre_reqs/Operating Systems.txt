What is an OS?
  OS is an interface between the Hardware and User Applications(Google Chrome, VSCode, etc.). It essentialy comprises of the Kernel, Drivers(Keyboard driver, Mourse driver, etc.) and System Applications(Bash Shell, File Explorer, etc.).

  Hardware typically comprises of:
  Central Processing Unit - CPU or Processor in short
  Memory
  Hard Drives
  Keyboard
  Mouse
  Monitors
  Printers

  The main functions of an OS are:
  Memory Management
  Process Management
  Disk Management

  The main OSes are Unix, Linux, Android, Mac OS, iOS, Windows. There's another special type of OS which is Real Time OS(RTOS) - It's generally used in IoT devices, embedded systems, Control systems. Such systems are majorly used by the Military and Space research folks. It involves a lot of communication with the hardware even for the basic stuff for performance reasons.

Architecture of CPU:
  A CPU consists of:
    Multiple Cores
    Memory Controller
    I/O Controller
    Cache

  Modern systems have got multiple cores in it such as quad(4) cores, octa(8) cores.
  The components of a core are as follows:
    Control Unit
    Arithmetic & Logic Unit - ALU
    Clock
    Registers
    Cache

What is the difference between a CPU and a GPU?
  Both are processing units of a computer, both have a Control Unit, Memory and a Core, although CPU is the main processor of the system, a Graphics Processing Unit - GPU is more specialized for more complex tasks and mathematical calculations. It's faster because it has got more ALUs in it. GPUs are specially designed for high throughput because it can run more operations in parallel in general than a regular CPU. A GPU can be considered as an assistant to a CPU, but it just has more processing power. It's like a master-slave architecture. GPUs were initially designed for smooth rendering of games on PCs and Gaming consoles because rendering games requires complex math calculations at a very high rate. But these days GPUs are also deployed on cloud infra servers which have got very high traffic for better processing and faster perfroamce on the cloud server.

Process v/s Thread v/s Program?
  Program:
  It's basically coding that we do. It's lines of instructions written in a particular programming language and executed in a specific order. It is an executable code. A program can spin up multiple processes. Spinning up new processes is also called as Process Spawning.

  Process:
  A process is actually an instance of a program in execution. A process occupies resources of a computer while executing, once executed the resources are freed/deallocated. A process has it's own Program Counter, Stack, variables and execution context(Program Control Block - PCB). A process terminates when the execution of the program is complete regardless if sucessful or not. Processes can be concurrent and communicate with each other through Inter Process Communication - IPC. IPC happens through a Bus Memory. A process can spin up multiple threads within itself. However typically a light weight process is single threaded, only heavy processes are multi threaded. Spinning up new threads is also called as Thread Spawning.

  Thread:
  A thread is an independent light weight unit or sequence of execution within a process that can be run in parallel with other threads within the same process. A process can have several threads and they can share resources amongst each other, that includes memory, compute, data and communication. Processes don't share resources, however threads within the same process can share resources if a process spins up multiple threads. A thread also has it's own Program Counter Stack.

What is Concurrent Execution?
  In one line it's - Full utilization and better performance. Concurrent execution of Threads, Programs or Processes ensures better utilization of the computer's resources. Thus improving the performance of the system to work faster and provide a better user experience.

How is a program executed?
  Suppose that we wanna play a game like Battlefield or Call of Duty, we know these are huge programs which can be 100 GB on the Hard drive/Secondary storage as well. We know that most computers have 16 GB RAM. Still we can play the game. So how does it work out to play a 100 GB game on a 16 GB computer? The answer is during execution of a process, the relevant resources are pulled from Secondary memory(Hard Drive) to Primary memory and the execution is started. There's something called Virtual Memory which plays out a role in such cases. Will study more on that later.

What's Multi Processing, Multi Programming, Multi Tasking and Multi Threading?
  Multi Processing:
  Multi Process refers to the execution of multiple processes on a system with multiple CPUs or CPU cores. Each process is an instance of a program and multiple processes can execute concurrently.

  Multi Programming:
  Multi Programming is the technique where multiple programs are loaded into the Computer memory at the same time for concurrent execution. The CPU switches between these multiple programs to execute instructions. This switching between programs facilitates maximum CPU utilization, suppose that one program is waiting for I/O then the other program is picked up for execution. This will keep the CPU continuously busy. Each program has got it's own memory space.

  Multi Threading:
  Multi Threading is done with the motto of dividing tasks of a process into smaller units of work that can be executed concurrently.

  Multi Tasking:
  Multi Tasking allows execution of multiple tasks, processes and programs to run in parallel on a Single Core. The Core's clock time is divided and swicthed rapidly between processes. Thus higher clock speeds are considered better for a faster core and CPU performance. Modern OSes use Multi Tasking on almost every second when from the time when the system is turned on until it's turned off.

Process Execution LifeCycle:
  The OS has complete control over how a process must be executed and in which order must it be picked up and in which state a particular process must be put in.
  Typical Process Execution LifeCycle is:
    New => Ready => Run => Termination
  But there are intermediate states involed in the LifeCycle as well, such states might be introduced based on the situation that the process is in.
  For Example, a process might halt it's execution temporarily(be suspended or in wait state) if another High Priority process might require the critical resources of CPU such as time and compute. Also, it might halt or suspend it's execution if it's waiting for I/O from the user or from the network. After the wait is over the process is picked back up for execution. If the wait time is too long then it might be killed altogether by the system itself. Such as a request with 408 status code.

  A process for whether to pick up for execution depends on several things; such as priority, wait queue, suspend state, scheduling algo, etc.
  Additional States of a Process in execution LifeCycle are:
    Wait/Block - Wait for I/O, or wait for another process in execution to be over
    Suspend/Wait - Suspended and moved to Wait Queue
    Suspend/Ready - Suspended but moved to Ready Queue

  Things involved in Process execution LifeCycle are:
    Scheduling Algorithm
    Priority Queue
    Wait Queue
    Suspend Queue

CPU Scheduling Algorithm:
  CPU scheduling algorithms are used by the OS to determine the order in which processes are executed on the CPU.
  Some of the well known algorithms are:

  First Come First Serve(FCFS):
  The meaning of the Algo is self explanatory in nature but it might lead to poor CPU utilization because in such a case a long process might occupy the entire time of the CPU and thus leave shorter processes too long in the Wait queue, also might abandon them altogether due to long wait times. It's also called Starvation in technical terms.

  Shortest Job Next(SJN) or Shortest Job First(SJF):
  SJF selects the process in which a process with the shortest execution time(burst time) is picked up first. This algo amins to minimize the average wait times and is suitable when Burst times are known in advance. Long processes might starve due to this approach.

  Round Robin(RR):
  This algo assigns a fixed time or Quantum for each process to execute and all the processes in the ready queue are given a chance for it's execution reagrdless of it's burst time known or unknown. Once a process exhausts it's quantum time from the CPU, then it's moved back to ready queue or wait queue depending upon the situation. But the disadvantage here is that it requires high amount of context switching and might be an overkill for long processes.

  Priority Scheduling:
  This assigns a priority all the processes, nataurally the processes with the highest priorities are executed first. Processes with the priority 0 is considered as highest priority. Subsequently processes with lower priority are executed. Due to which the currently executing process might have to move into Ready queue as per the situation. In this approach Lower priority processes might starve.

  Multilevel Queue Scheduling:
  Multilevel queue scheduling divides the ready queue into multiple priority queues eahc has it's own scheduling algorithms. Processes are initially placed in the highest priority queue and can move between queues based on a criteria of switching. Each queue can use a different scheduling algorithm suitable for the queue. But it's usually a combination of Round Robin along with Priority scheduling.

Critical Section Problem:
  This problem occurs when multiple processes are trying to modify the same or a shared resource. This might lead to loss of data consistency, integrity and relaibility.
  To maintain data integrity it must be ensured that only one process/thread must be allowed to modify a resource at the time. To achieve this it must be done in a mutually exclusive manner. Usually locks are applied on resources on which the actions are being performed upon. Generally Write locks are much more important than Read locks. Process synchronization plays a major role in this scenario.

Process Synchronization:
  This serves as a traffic signal to help regulate the flow of vehicles at an intersection. Process synchronization thet coordindate and communicate properly to avoid conflicts and proper order of execution of processes. It helps prevent issues like Race Conditions, Data Inconsistencies or Deadlocks when multiple processes access shared resources. We have to build a Process Synchronization mechanism to ensure smooth running of the system.
  The requirements for building a Process Synchronization mechanism are as follows:
    Mutual Exclusion - Mutex:
    Every mechanism should enforce Mutual Exclusion or Mutex in short for all the shared resources, which means that no other process or thread should access a shared resource when another process or thread is accessing it to avoid data inconsistency.

    Progress Tracking:
    Every process should make continuous progress and should not be executing indefinitely nor be halted in execution. The continuous progress ensures that we don't arrive to a deadlock situation where the current process is not leaving the control of a shared resource whereas the other processes are waiting to access that shared resource.

    Bounded Waiting:
    Every process should have a bounded waiting time so that it is not starved indefinitely. This ensures that every process gets a chance to access a shared resource.

Process Synchronization Mechanisms:

  Types of Locks:
    Read Lock
    Write Lock
    Mutex
    Semaphore

  Deadlocks & Deadlock Avoidance:
  Deadlocks means that the system can't move further and will have to be shutdown. The following are the necessary conditions for the deadlock to occur:
    Mutual Exclusion
    Hold and Wait
    No preemption
    Circular Wait

  Deadlock Handling Techniques:
    Deadlock Prevention
    Deadlock Avoidance
    Deadlock Detection
    Deadlock Recovery
    Deadlock Ignorance
  But most systems like Linux and Windows focus more on performance than preventing Deadlocks, because most sytems are used for personal use.

FUN QUESTION ABOUT INCREASING RAM OR MEMORY:
Why is it advised that we increase the RAM whenever our computer becomes slow?
It's because when we increase the RAM it doesn't mean that our CPU has become faster, thus the computer has become faster. The CPU speed depends on the Clock speed. But instead of CPU becoming faster the degree of Multi Programming has increased due to availability of more memory when we increase the RAM. The processor can now use more memory to execute more processes in parallel. In collaboration of Virtual Memory and Main Memory the computer becomes faster.

Memory Management:
  Fixed partitioning and dynamic partitioning are the techniques used for allocation of memory and manage resources.

  Fragmentation:
  Unused space in memory.

  Fixed Partitioning:
  In fixed partitioning the memory is divided into fixed size partitions or blocks and each partition is assigned a specific process or task. That partition remains fixed throughout the execution. Fixed partition is relatively simple to implement but causes a lot of memory to be unused and thus wasted. This is known as Internal Fragmentation.

  Dynamic Partitioning:
  This is also called as variable partitioning, it allows memory to be allocated and deallocated dynamically based on size requirements of processes. This solves the problem of Internal Fragmentation to a great extent, but managing fragmentation and efficiently allocating and deallocating memory can be more complex. But a problem over here is that it may lead to External Fragmentation, which means that when a process is unlaoded from the memory then a new process of a larger size will not be to be loaded within the same fragment as that of the deallocated process. Thus it will lead to memory wastage again.
  The problem of Internal and External fragmentation is solved through Paging Algorithms, we'll study that later.
  Techniques like first-fit, best-fit and worst fit are used in systems which use Dynamic Partitioning.

    First Fit: Allocates the first available memory block that is large enough to accomodate the process. It starts searching the memory and allocates the first available block suitable for the process.
    Best Fit: This algo also searches the memory and allocates a block which is equivalent or a little larger than the process and then allocate it there.
    Worst Fit: It finds the biggest space and then starts dumping all the processes in it.

  Paging - Revolutionary:
  Better than Dynamic Partitioning.
  It's a storage mechanism that is used to retrieve processes from Secondary Storage into the Main Memory in the form of Pages. The main idea is to divide the memory in the form of Pages. The main memory will also be divided in the form of Frames. One page of the process can be stored in the frames of the memory. The pages can be stored in different places in memory but the priority is to always find out contiguous frames of memory. Typical size of pages is 1KB for instance.
    Page Table:
    Pages of the process are brought into the main memory only when they are required otherwise they reside in the secondary storage.
    Pages are managed by the Memory Management Unit(MMU). The MMU basically maps the pages and addresses of the memory locations on which a process is currently residing and executing. This is called as Page Table.
    The Program counter executes in a serial manner and works in coordindation of the Page Table for getting the Physical addresses of the memory locations.
    The Page Table has got 2 columns, the Logical address column and the Physical address column. The Program counter reaches the Logical address and accesses the Physical address from the memory for execution.

  Virtual Memory:
  Virtual memory is a OS concept that lets computer use more memory than it actually has. It creates an imaginary memory space that combines Main Memory and Secondary memory. When a program needs more memory than is available on the RAM, it moves some data temporarily to Secondary Storage. This allows the computer to run larger programs and handle multiple tasks at once. Virtual Memory also provides Memory protection.
  Instead of loading one big process in the main memory the OS loads different parts of more than one process in the main memory. By doing this Multi Programming will increase and therefore the CPU utilization will also be increased.

  Demand Paging:

  Segmentation:
  Better than Paging.

Concepts of OS:
  System Calls:
    Process System Calls:
      fork()
      wait()
      exit()
    File Management System Calls:
      open()
      read()
      write()
      close()
      chmod()
      unmask()
    Information Management:
      getpid()
      alarm()
      sleep()

  System Initialization Process:
    UEFI
    Boot Loader
    GRUB

  Memory Allocation Methods:
    Segmentation
    Demand Paging
    Page Faults
